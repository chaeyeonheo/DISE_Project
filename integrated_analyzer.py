"""
í†µí•© ìˆ˜ë©´ ë‚´ì‹œê²½ ë¹„ë””ì˜¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (Fixed: Proper Crop + Color-based ROI)
"""

import cv2
import numpy as np
from pathlib import Path
from datetime import timedelta
import json
from tqdm import tqdm
import torch
from torchvision import transforms
from PIL import Image
import sys

# Classification ëª¨ë¸ import
sys.path.append(str(Path(__file__).parent / 'ote_velum_classification_final'))
from model import create_model

class IntegratedDISEAnalyzer:
    def __init__(self, 
                 model_path,
                 fps_extract=1,
                 threshold_percent=10,
                 exclude_first_seconds=0,
                 exclude_last_seconds=0,
                 min_event_duration=1.0,
                 manual_max_area=None):
        
        self.fps_extract = fps_extract
        self.threshold_percent = threshold_percent
        self.exclude_first_seconds = exclude_first_seconds
        self.exclude_last_seconds = exclude_last_seconds
        self.min_event_duration = min_event_duration
        self.manual_max_area = manual_max_area
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"DEVICE: {self.device}")
        
        try:
            self.model = self._load_classification_model(model_path)
            print("âœ… Classification ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
            
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        self.class_names = ['OTE', 'Velum', 'None'] 
        
        self.results = {
            'video_info': {},
            'frame_classifications': [],
            'segments': [],
            'occlusion_events': [],
            'summary': {},
            'max_area': manual_max_area if manual_max_area else 0,
            'max_area_frame': 0,
            'max_area_source': 'manual' if manual_max_area else 'auto'
        }
    
    def _load_classification_model(self, model_path):
        model = create_model('resnet50', num_classes=3, pretrained=False)
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)
        model.eval()
        return model
    
    def preprocess_frame(self, frame):
        """í”„ë ˆì„ ì „ì²˜ë¦¬ - ê²€ì€ ë°°ê²½ ì œê±°í•˜ê³  ë‚´ì‹œê²½ ì˜ì—­ë§Œ CROP"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        _, binary = cv2.threshold(gray, 20, 255, cv2.THRESH_BINARY)
        
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=3)
        
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            
            # ì•ˆìª½ìœ¼ë¡œ ì—¬ìœ  ë‘ê¸° (vignetting ì œê±°)
            margin_percent = 0.15  # 15% ì•ˆìª½ìœ¼ë¡œ
            margin_x = int(w * margin_percent)
            margin_y = int(h * margin_percent)
            
            x = max(0, x + margin_x)
            y = max(0, y + margin_y)
            w = min(frame.shape[1] - x, w - 2 * margin_x)
            h = min(frame.shape[0] - y, h - 2 * margin_y)
            
            # âœ… í•µì‹¬: ë‚´ì‹œê²½ ì˜ì—­ë§Œ ì˜ë¼ë‚´ê¸° (ê²€ì€ ë°°ê²½ ì—†ìŒ)
            cropped = frame[y:y+h, x:x+w].copy()
            return cropped, (x, y, w, h)
        
        return frame, (0, 0, frame.shape[1], frame.shape[0])
    
    def classify_frame(self, frame):
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)
        input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probs = torch.softmax(outputs, dim=1)
            pred_idx = torch.argmax(probs, dim=1).item()
            confidence = probs[0][pred_idx].item()
        return self.class_names[pred_idx], confidence
    
    def analyze_roi_dual_track(self, frame, label):
        """Color-based ROI íƒì§€ (ì´ë¯¸ cropëœ í”„ë ˆì„ì—ì„œ ì–´ë‘ìš´ ê¸°ë„ ì˜ì—­ ì°¾ê¸°)"""
        if label == 'None':
            return 0, None
        
        if frame is None or frame.size == 0:
            return 0, None
        
        # HSV ë³€í™˜
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # ì–´ë‘ìš´ ì˜ì—­ íƒ€ê²ŸíŒ…
        lower_dark = np.array([0, 0, 0])
        upper_dark = np.array([180, 255, 80])
        
        mask = cv2.inRange(hsv, lower_dark, upper_dark)
        
        # Morphological operations
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=3)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)
        
        # ê°€ì¥ í° ì—°ê²° ì˜ì—­
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            mask, connectivity=8
        )
        
        if num_labels <= 1:
            return 0, None
        
        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
        area = float(stats[largest_label, cv2.CC_STAT_AREA])
        mask_final = (labels == largest_label).astype('uint8') * 255
        
        # ì¤‘ì‹¬ì´ ì´ë¯¸ì§€ ì¤‘ì•™ ê·¼ì²˜ì— ìˆëŠ”ì§€ í™•ì¸ (Velumì€ ë” ê´€ëŒ€í•˜ê²Œ)
        h, w = frame.shape[:2]
        center_x, center_y = centroids[largest_label]
        img_center_x, img_center_y = w / 2, h / 2
        distance_from_center = np.sqrt((center_x - img_center_x)**2 + (center_y - img_center_y)**2)
        
        # Velumì€ ì¤‘ì‹¬ ê²€ì¦ì„ ë” ê´€ëŒ€í•˜ê²Œ (50% vs 45%)
        max_distance_ratio = 0.50 if label == 'Velum' else 0.45
        max_distance = min(w, h) * max_distance_ratio
        
        if distance_from_center > max_distance:
            return 0, None
        
        # OTE ë ˆì´ë¸” íŠ¹í™” í•„í„°ë§ë§Œ ì ìš©
        if label == 'OTE':
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 20, 80)
            edge_score = np.mean(edges)
            if edge_score > 12.0 and area < 3000:
                return 0, None
            roi_pixels = gray[mask_final > 0]
            if len(roi_pixels) > 0 and np.std(roi_pixels) > 50 and area < 1000:
                return 0, None

        return area, mask_final

    def detect_segments(self, frame_classifications):
        if not frame_classifications: return []
        smoothed_labels = [f['label'] for f in frame_classifications]
        for i in range(1, len(smoothed_labels) - 1):
            if smoothed_labels[i-1] == smoothed_labels[i+1] and smoothed_labels[i] != smoothed_labels[i-1]:
                smoothed_labels[i] = smoothed_labels[i-1]
                frame_classifications[i]['label'] = smoothed_labels[i-1]
        
        segments = []
        current_segment = None
        for i, frame_data in enumerate(frame_classifications):
            label = frame_data['label']
            if label in ['OTE', 'Velum']:
                if current_segment is None:
                    current_segment = {'label': label, 'start_frame': frame_data['frame_number'], 'start_time': frame_data['timestamp'], 'frames': [frame_data]}
                elif current_segment['label'] == label:
                    current_segment['frames'].append(frame_data)
                else:
                    current_segment['end_frame'] = current_segment['frames'][-1]['frame_number']
                    current_segment['end_time'] = current_segment['frames'][-1]['timestamp']
                    current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
                    segments.append(current_segment)
                    current_segment = {'label': label, 'start_frame': frame_data['frame_number'], 'start_time': frame_data['timestamp'], 'frames': [frame_data]}
            else:
                if current_segment is not None:
                    current_segment['end_frame'] = current_segment['frames'][-1]['frame_number']
                    current_segment['end_time'] = current_segment['frames'][-1]['timestamp']
                    current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
                    segments.append(current_segment)
                    current_segment = None
        
        if current_segment is not None:
            current_segment['end_frame'] = current_segment['frames'][-1]['frame_number']
            current_segment['end_time'] = current_segment['frames'][-1]['timestamp']
            current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
            segments.append(current_segment)
        
        # âœ… ë ˆì´ë¸”ë³„ë¡œ ì „ì²´ í”„ë ˆì„ì—ì„œ max_area ê³„ì‚°
        valid_segments = [s for s in segments if s['duration'] > 0.5]
        
        label_max_areas = {}  # {'OTE': max_area, 'Velum': max_area}
        
        for label in ['OTE', 'Velum']:
            # í•´ë‹¹ ë ˆì´ë¸”ì˜ ëª¨ë“  í”„ë ˆì„ ìˆ˜ì§‘
            label_frames = [f for f in frame_classifications if f['label'] == label]
            label_areas = [f['roi_area'] for f in label_frames if f.get('roi_area', 0) > 0]
            
            if label_areas:
                label_max_areas[label] = max(label_areas)
                # í•´ë‹¹ max_areaë¥¼ ê°€ì§„ í”„ë ˆì„ ì°¾ê¸°
                max_frame = max(label_frames, key=lambda f: f.get('roi_area', 0))
                print(f"  ğŸ“ {label} global max_area = {label_max_areas[label]:.0f} pxÂ² (frame {max_frame['frame_number']})")
            else:
                label_max_areas[label] = 0
        
        # ê° segmentì— í•´ë‹¹ ë ˆì´ë¸”ì˜ max_area í• ë‹¹
        for segment in valid_segments:
            segment['max_area'] = label_max_areas.get(segment['label'], 0)
            # max_area_frameì€ ì „ì²´ ë ˆì´ë¸”ì˜ max_area í”„ë ˆì„
            if segment['max_area'] > 0:
                label_frames = [f for f in frame_classifications if f['label'] == segment['label']]
                max_frame = max(label_frames, key=lambda f: f.get('roi_area', 0))
                segment['max_area_frame'] = max_frame['frame_number']
            else:
                segment['max_area_frame'] = segment['start_frame']
            
        return valid_segments
    
    def detect_occlusion_events(self, segments):
        """Segmentë³„ max_area ê¸°ì¤€ìœ¼ë¡œ íì‡„ ì´ë²¤íŠ¸ ê°ì§€"""
        events = []
        
        for segment in segments:
            segment_max_area = segment.get('max_area', 0)
            
            if segment_max_area < 1000:
                print(f"âš ï¸ {segment['label']} segment max_area ({segment_max_area:.0f}) too small, skipping")
                continue
            
            # ì´ segmentì˜ threshold
            threshold_area = segment_max_area * (1 - self.threshold_percent / 100)
            print(f"  ğŸ¯ {segment['label']} threshold: {threshold_area:.0f} pxÂ² ({self.threshold_percent}% of {segment_max_area:.0f})")
            
            current_event = None
            
            for frame_data in segment['frames']:
                roi_area = frame_data.get('roi_area', 0)
                
                # Segment ê¸°ì¤€ìœ¼ë¡œ ê°ì†Œìœ¨ ê³„ì‚°
                if roi_area > 0:
                    frame_data['reduction_percent'] = (1 - roi_area / segment_max_area) * 100
                else:
                    frame_data['reduction_percent'] = 100.0
                
                # âœ… ìˆ˜ì •: roi_areaê°€ thresholdë³´ë‹¤ ì‘ê±°ë‚˜ 0ì¸ ê²½ìš° ëª¨ë‘ ì´ë²¤íŠ¸ë¡œ ê°„ì£¼
                is_occlusion = (roi_area < threshold_area)  # 0ë„ í¬í•¨
                
                if is_occlusion:
                    area_reduction = (1 - roi_area / segment_max_area) * 100 if roi_area > 0 else 100.0
                    severity = self._classify_severity(area_reduction)
                    if current_event is None:
                        current_event = {
                            'segment_label': segment['label'], 
                            'segment_max_area': segment_max_area,
                            'start_frame': frame_data['frame_number'], 
                            'start_time': frame_data['timestamp'], 
                            'severity': severity, 
                            'max_reduction': area_reduction,
                            'frames': [frame_data]
                        }
                    else:
                        current_event['frames'].append(frame_data)
                        current_event['max_reduction'] = max(current_event['max_reduction'], area_reduction)
                        if self._severity_level(severity) > self._severity_level(current_event['severity']):
                            current_event['severity'] = severity
                else:
                    if current_event is not None:
                        current_event['end_frame'] = current_event['frames'][-1]['frame_number']
                        current_event['end_time'] = current_event['frames'][-1]['timestamp']
                        current_event['duration'] = current_event['end_time'] - current_event['start_time']
                        if current_event['duration'] >= self.min_event_duration: 
                            events.append(current_event)
                            print(f"    âœ“ Event detected: {current_event['severity']} at {current_event['start_time']:.1f}s-{current_event['end_time']:.1f}s ({current_event['max_reduction']:.1f}% reduction)")
                        else:
                            print(f"    âœ— Event too short: {current_event['duration']:.2f}s < {self.min_event_duration}s")
                        current_event = None
            
            if current_event is not None:
                current_event['end_frame'] = current_event['frames'][-1]['frame_number']
                current_event['end_time'] = current_event['frames'][-1]['timestamp']
                current_event['duration'] = current_event['end_time'] - current_event['start_time']
                if current_event['duration'] >= self.min_event_duration: 
                    events.append(current_event)
                    print(f"    âœ“ Event detected (at end): {current_event['severity']} at {current_event['start_time']:.1f}s-{current_event['end_time']:.1f}s ({current_event['max_reduction']:.1f}% reduction)")
                else:
                    print(f"    âœ— Event too short (at end): {current_event['duration']:.2f}s < {self.min_event_duration}s")
        
        return events
    
    def _classify_severity(self, reduction_percent):
        if reduction_percent >= 70: return 'Critical'
        if reduction_percent >= 50: return 'Severe'
        if reduction_percent >= 30: return 'Moderate'
        return 'Mild'
    
    def _severity_level(self, severity):
        return {'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Critical': 4}.get(severity, 0)
    
    def _create_segment_reference_images(self, video_path, output_dir):
        """ê° segmentë³„ reference ì´ë¯¸ì§€ ìƒì„± (OTE, Velum ê°ê°)"""
        cap = cv2.VideoCapture(str(video_path))
        
        for segment in self.results['segments']:
            if segment.get('max_area', 0) == 0:
                continue
                
            max_area_frame = segment.get('max_area_frame', 0)
            if max_area_frame == 0:
                continue
            
            # ë¹„ë””ì˜¤ì—ì„œ í•´ë‹¹ í”„ë ˆì„ ì½ê¸°
            cap.set(cv2.CAP_PROP_POS_FRAMES, max_area_frame)
            ret, frame = cap.read()
            if not ret:
                continue
            
            # ì „ì²˜ë¦¬ (ë‚´ì‹œê²½ ì˜ì—­ë§Œ crop)
            preprocessed, bbox = self.preprocess_frame(frame)
            
            # ROI íƒì§€
            label = segment['label']
            _, roi_mask = self.analyze_roi_dual_track(preprocessed, label)
            
            overlay = preprocessed.copy()
            
            # ROI ìœ¤ê³½ì„ ë§Œ í‘œì‹œ
            if roi_mask is not None:
                contours, _ = cv2.findContours(roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(overlay, contours, -1, (0, 255, 255), 4)
            
            # segment ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
            cv2.rectangle(overlay, (5, 5), (overlay.shape[1]-5, 100), (0, 0, 0), -1)
            cv2.rectangle(overlay, (5, 5), (overlay.shape[1]-5, 100), (255, 255, 255), 2)
            
            segment_color = (255, 255, 0) if label == 'OTE' else (255, 0, 255)
            cv2.putText(overlay, f"Reference: {label}", (15, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, segment_color, 2)
            cv2.putText(overlay, f"Frame: {max_area_frame}", (15, 55), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(overlay, f"Max Area: {segment['max_area']:.0f} pxÂ²", (15, 75), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(overlay, f"Time: {segment['start_time']:.1f}s - {segment['end_time']:.1f}s", (15, 95), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)
            
            ref_path = Path(output_dir) / "overlays" / f"reference_{label}.jpg"
            ref_path.parent.mkdir(parents=True, exist_ok=True)
            cv2.imwrite(str(ref_path), overlay)
            
            # resultsì— ì €ì¥
            if 'reference_images' not in self.results:
                self.results['reference_images'] = {}
            self.results['reference_images'][label] = str(ref_path)
            
            print(f"âœ… {label} Reference ì´ë¯¸ì§€ ìƒì„±: {ref_path}")
        
        cap.release()
    
    def _create_debug_frames(self, video_path, output_dir):
        """ê° í”„ë ˆì„ë³„ë¡œ ROI ìœ¤ê³½ì„ ê³¼ SEGMENT ì •ë³´ë¥¼ í‘œì‹œí•œ ë””ë²„ê¹… ì´ë¯¸ì§€ ìƒì„±"""
        debug_dir = Path(output_dir) / "debug_frames"
        debug_dir.mkdir(parents=True, exist_ok=True)
        
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        reference_max_area = self.results['max_area']
        
        saved_count = 0
        for frame_data in tqdm(self.results['frame_classifications'], desc="Debug frames"):
            frame_num = frame_data['frame_number']
            label = frame_data['label']
            roi_area = frame_data.get('roi_area', 0)
            timestamp = frame_data['timestamp']
            
            # í•´ë‹¹ í”„ë ˆì„ ì½ê¸°
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
            ret, frame = cap.read()
            if not ret:
                continue
            
            # ì „ì²˜ë¦¬ (ë‚´ì‹œê²½ ì˜ì—­ë§Œ crop)
            preprocessed, bbox = self.preprocess_frame(frame)
            
            # ROI íƒì§€
            roi_mask = None
            if label in ['OTE', 'Velum']:
                _, roi_mask = self.analyze_roi_dual_track(preprocessed, label)
            
            # ë””ë²„ê¹… ì´ë¯¸ì§€ëŠ” preprocessed ê¸°ì¤€
            debug_frame = preprocessed.copy()
            
            # ROI ìœ¤ê³½ì„ ë§Œ í‘œì‹œ
            if roi_mask is not None and roi_area > 0:
                contours, _ = cv2.findContours(roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(debug_frame, contours, -1, (0, 255, 255), 3)
            
            # í•´ë‹¹ í”„ë ˆì„ì´ ì†í•œ segment ì°¾ê¸°
            current_segment = None
            for segment in self.results['segments']:
                if segment['start_frame'] <= frame_num <= segment['end_frame']:
                    current_segment = segment
                    break
            
            # Reduction ê³„ì‚°
            reduction = (1 - roi_area / reference_max_area) * 100 if (reference_max_area > 0 and roi_area > 0) else 0
            
            # ì •ë³´ íŒ¨ë„ (ìƒë‹¨)
            info_height = 140
            cv2.rectangle(debug_frame, (5, 5), (debug_frame.shape[1]-5, info_height), (0, 0, 0), -1)
            cv2.rectangle(debug_frame, (5, 5), (debug_frame.shape[1]-5, info_height), (255, 255, 255), 2)
            
            # í”„ë ˆì„ ì •ë³´
            font_scale = 0.7
            thickness = 2
            y_offset = 25
            line_height = 25
            
            cv2.putText(debug_frame, f"Frame: {frame_num} | Time: {timestamp:.2f}s", (15, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
            y_offset += line_height
            
            # Label ì •ë³´
            label_color = (0, 255, 255) if label == 'OTE' else (255, 0, 255) if label == 'Velum' else (128, 128, 128)
            cv2.putText(debug_frame, f"Label: {label}", (15, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, label_color, thickness)
            y_offset += line_height
            
            # ROI ì •ë³´
            if roi_area > 0:
                cv2.putText(debug_frame, f"ROI Area: {roi_area:.0f} pxÂ²", (15, y_offset), 
                           cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 255), thickness)
                y_offset += line_height
                cv2.putText(debug_frame, f"Reduction: {reduction:.1f}%", (15, y_offset), 
                           cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 255), thickness)
            else:
                cv2.putText(debug_frame, f"ROI Area: N/A", (15, y_offset), 
                           cv2.FONT_HERSHEY_SIMPLEX, font_scale, (128, 128, 128), thickness)
            
            # Segment ì •ë³´ (í•˜ë‹¨)
            if current_segment:
                segment_y = debug_frame.shape[0] - 80
                cv2.rectangle(debug_frame, (5, segment_y), (debug_frame.shape[1]-5, debug_frame.shape[0]-5), (0, 0, 0), -1)
                cv2.rectangle(debug_frame, (5, segment_y), (debug_frame.shape[1]-5, debug_frame.shape[0]-5), (255, 255, 0), 3)
                
                segment_color = (255, 255, 0) if current_segment['label'] == 'OTE' else (255, 0, 255)
                cv2.putText(debug_frame, f"SEGMENT: {current_segment['label']}", (15, segment_y + 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, segment_color, 2)
                cv2.putText(debug_frame, f"Time: {current_segment['start_time']:.1f}s - {current_segment['end_time']:.1f}s", (15, segment_y + 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            else:
                segment_y = debug_frame.shape[0] - 50
                cv2.rectangle(debug_frame, (5, segment_y), (debug_frame.shape[1]-5, debug_frame.shape[0]-5), (0, 0, 0), -1)
                cv2.putText(debug_frame, "SEGMENT: None", (15, segment_y + 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (128, 128, 128), 2)
            
            # íŒŒì¼ ì €ì¥
            filename = f"frame_{frame_num:06d}_t{timestamp:.2f}s_{label}.jpg"
            filepath = debug_dir / filename
            cv2.imwrite(str(filepath), debug_frame)
            saved_count += 1
        
        cap.release()
        print(f"âœ… ë””ë²„ê¹… ì´ë¯¸ì§€ {saved_count}ê°œ ì €ì¥ ì™„ë£Œ: {debug_dir}")
    
    def create_event_clips(self, video_path, events, output_dir):
        """Side-by-Side ë¹„ë””ì˜¤: ìš°ì¸¡ì— ROI ìœ¤ê³½ì„  + segment ì •ë³´ í‘œì‹œ"""
        output_dir = Path(output_dir)
        clips_dir = output_dir / 'event_clips'
        clips_dir.mkdir(parents=True, exist_ok=True)
        
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        w_orig = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        h_orig = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # H.264 ì½”ë± ì‚¬ìš© (ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ê°œì„ )
        fourcc = cv2.VideoWriter_fourcc(*'avc1')  # H.264
        ext = 'mp4'
        print(f"ğŸ¥ Using H.264 codec for better browser compatibility")

        segments = self.results['segments']

        for i, event in enumerate(events):
            start_frame = max(0, event['start_frame'] - int(fps * 1.0))
            end_frame = event['end_frame'] + int(fps * 1.0)
            
            filename = f"event_{i+1:02d}_{event['segment_label']}_{event['severity']}.{ext}"
            filepath = clips_dir / filename
            
            # ì²« í”„ë ˆì„ìœ¼ë¡œ í¬ê¸° ê²°ì •
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            ret, first_frame = cap.read()
            if not ret:
                continue
                
            preprocessed_first, _ = self.preprocess_frame(first_frame)
            h_crop, w_crop = preprocessed_first.shape[:2]
            
            # Side-by-side í¬ê¸°
            out_w = w_crop * 2
            out_h = h_crop
            
            out = cv2.VideoWriter(str(filepath), fourcc, fps, (out_w, out_h))
            if not out.isOpened():
                print(f"âš ï¸ H.264 ì½”ë± ì‹¤íŒ¨, mp4vë¡œ ì¬ì‹œë„: {filepath}")
                fourcc_fallback = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(str(filepath), fourcc_fallback, fps, (out_w, out_h))
                if not out.isOpened():
                    print(f"âŒ ë¹„ë””ì˜¤ Writer ì´ˆê¸°í™” ì‹¤íŒ¨: {filepath}")
                    continue
            
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            frame_count = 0
            
            # í•´ë‹¹ ì´ë²¤íŠ¸ì˜ segment ì •ë³´
            event_segment = None
            for segment in segments:
                if segment['label'] == event['segment_label'] and \
                   segment['start_frame'] <= event['start_frame'] <= segment['end_frame']:
                    event_segment = segment
                    break
            
            for f_idx in range(start_frame, end_frame + 1):
                ret, frame = cap.read()
                if not ret: break
                
                # ì „ì²˜ë¦¬
                preprocessed, bbox = self.preprocess_frame(frame)
                
                left = preprocessed.copy()
                right = preprocessed.copy()
                
                # í˜„ì¬ í”„ë ˆì„ì´ ì†í•œ segment ì°¾ê¸°
                current_segment = None
                for segment in segments:
                    if segment['start_frame'] <= f_idx <= segment['end_frame']:
                        current_segment = segment
                        break
                
                # âœ… ì‹¤ì‹œê°„ ROI ê³„ì‚° ë° reduction ê³„ì‚°
                reduction = 0
                if current_segment and current_segment['label'] in ['OTE', 'Velum']:
                    roi_area_current, roi_mask = self.analyze_roi_dual_track(preprocessed, current_segment['label'])
                    
                    # ROI ìœ¤ê³½ì„  í‘œì‹œ
                    if roi_mask is not None:
                        contours, _ = cv2.findContours(roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                        cv2.drawContours(right, contours, -1, (0, 255, 255), 3)
                    
                    # Reduction ê³„ì‚° (segmentì˜ max_area ê¸°ì¤€)
                    segment_max_area = current_segment.get('max_area', 0)
                    if segment_max_area > 0:
                        if roi_area_current > 0:
                            reduction = (1 - roi_area_current / segment_max_area) * 100
                        else:
                            reduction = 100.0
                
                # ì´ë²¤íŠ¸ êµ¬ê°„ ì—¬ë¶€
                is_event = event['start_frame'] <= f_idx <= event['end_frame']
                
                # ìƒë‹¨ ì •ë³´
                info_height = 95
                cv2.rectangle(right, (5, 5), (w_crop-5, info_height), (0, 0, 0), -1)
                cv2.rectangle(right, (5, 5), (w_crop-5, info_height), (255, 255, 255), 2)
                
                text_color = (0, 0, 255) if is_event else (255, 255, 255)
                font_scale = 1.0
                thickness = 2
                
                if current_segment:
                    cv2.putText(right, f"Segment: {current_segment['label']}", (15, 35), 
                               cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, thickness)
                    cv2.putText(right, f"Max: {current_segment.get('max_area', 0):.0f} pxÂ²", (15, 65), 
                               cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, thickness)
                    cv2.putText(right, f"Reduction: {reduction:.1f}%", (15, 90), 
                               cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, thickness)
                else:
                    cv2.putText(right, "Segment: None", (15, 35), 
                               cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, thickness)
                
                # ì´ë²¤íŠ¸ êµ¬ê°„ í‘œì‹œ (í•˜ë‹¨)
                if is_event:
                    event_height = 85
                    cv2.rectangle(right, (5, h_crop-event_height), (w_crop-5, h_crop-5), (0, 0, 0), -1)
                    cv2.rectangle(right, (5, h_crop-event_height), (w_crop-5, h_crop-5), (0, 0, 255), 4)
                    cv2.putText(right, f"OCCLUSION EVENT: {event['severity']}", (15, h_crop-45), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)
                    cv2.putText(right, f"Reduction: {reduction:.1f}%", (15, h_crop-20), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
                
                combined = np.hstack((left, right))
                out.write(combined)
                frame_count += 1
            
            out.release()
            if filepath.exists() and filepath.stat().st_size > 0:
                print(f"âœ… í´ë¦½ ìƒì„± ì™„ë£Œ: {filename} ({frame_count} í”„ë ˆì„)")
                event['clip_path'] = f"event_clips/{filename}"
            else:
                print(f"âŒ í´ë¦½ ìƒì„± ì‹¤íŒ¨: {filename}")
                
        cap.release()

    def analyze_video(self, video_path, output_dir=None):
        video_path = Path(video_path)
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        start_frame = int(fps * self.exclude_first_seconds)
        end_frame = total_frames - int(fps * self.exclude_last_seconds)
        
        self.results['video_info'] = {'filename': video_path.name, 'fps': fps, 'total_frames': total_frames, 'duration': duration}
        print(f"\nğŸš€ ë¶„ì„ ì‹œì‘: {video_path.name}")
        
        if self.manual_max_area:
            print(f"ğŸ“Œ Manual max_area ì‚¬ìš©: {self.manual_max_area:.0f} pxÂ²")
        
        interval = max(1, int(fps / self.fps_extract))
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        curr_frame = start_frame
        
        pbar = tqdm(total=(end_frame - start_frame), desc="Processing")
        
        while curr_frame < end_frame:
            ret, frame = cap.read()
            if not ret: break
            
            if (curr_frame - start_frame) % interval == 0:
                preprocessed, bbox = self.preprocess_frame(frame)
                label, confidence = self.classify_frame(preprocessed)
                roi_area = 0
                if label in ['OTE', 'Velum']:
                    roi_area, _ = self.analyze_roi_dual_track(preprocessed, label)
                
                if not self.manual_max_area:
                    if roi_area > self.results['max_area']:
                        self.results['max_area'] = roi_area
                        self.results['max_area_frame'] = curr_frame

                timestamp = curr_frame / fps
                self.results['frame_classifications'].append({
                    'frame_number': curr_frame, 'timestamp': timestamp,
                    'label': label, 'roi_area': roi_area
                })
                
            curr_frame += 1
            pbar.update(1)
        pbar.close()
        cap.release()
        
        print(f"ğŸ“Š Reference max_area: {self.results['max_area']:.0f} pxÂ² ({self.results['max_area_source']})")
        
        print("ğŸ” í›„ì²˜ë¦¬ ì¤‘ (êµ¬ê°„ ë³‘í•© ë° ì´ë²¤íŠ¸ ê°ì§€)...")
        self.results['segments'] = self.detect_segments(self.results['frame_classifications'])
        
        # âœ… segment_references ì €ì¥ (ë¦¬í¬íŠ¸ìš©)
        self.results['segment_references'] = {}
        for segment in self.results['segments']:
            if segment.get('max_area', 0) > 0:
                self.results['segment_references'][segment['label']] = {
                    'max_area': segment['max_area'],
                    'frame_number': segment['max_area_frame']
                }
        
        self.results['occlusion_events'] = self.detect_occlusion_events(self.results['segments'])
        
        self.results['metadata'] = {'threshold_percent': self.threshold_percent}
        
        self.results['summary'] = {
            'total_segments': len(self.results['segments']),
            'ote_segments': sum(1 for s in self.results['segments'] if s['label'] == 'OTE'),
            'velum_segments': sum(1 for s in self.results['segments'] if s['label'] == 'Velum'),
            'total_events': len(self.results['occlusion_events']),
            'events_by_severity': {
                'Critical': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Critical'),
                'Severe': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Severe'),
                'Moderate': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Moderate'),
                'Mild': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Mild'),
            }
        }
        
        if output_dir:
            out_path = Path(output_dir)
            out_path.mkdir(parents=True, exist_ok=True)
            
            # Segmentë³„ reference ì´ë¯¸ì§€ ìƒì„±
            if self.results['segments']:
                print("ğŸ“¸ Segmentë³„ Reference ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
                self._create_segment_reference_images(video_path, output_dir)
            
            print("ğŸ” ë””ë²„ê¹…ìš© í”„ë ˆì„ë³„ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            self._create_debug_frames(video_path, output_dir)
            
            if self.results['occlusion_events']:
                print("ğŸ¥ ì´ë²¤íŠ¸ í´ë¦½ ìƒì„± ì¤‘...")
                self.create_event_clips(video_path, self.results['occlusion_events'], output_dir)
            
            with open(out_path / 'analysis_results.json', 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
                
        print("âœ… ë¶„ì„ ì™„ë£Œ!")
        return self.results