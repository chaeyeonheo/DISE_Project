"""
í†µí•© ìˆ˜ë©´ ë‚´ì‹œê²½ ë¹„ë””ì˜¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸
1. OTE/Velum/None ë¶„ë¥˜
2. OTE/Velum êµ¬ê°„ì—ì„œë§Œ íìƒ‰ ë¶„ì„
3. ì—°ì† êµ¬ê°„ ê°ì§€
4. ìœ„í—˜ êµ¬ê°„ ë¹„ë””ì˜¤ í´ë¦½ ìƒì„±
"""

import cv2
import numpy as np
from pathlib import Path
from datetime import timedelta
import json
from tqdm import tqdm
import torch
from torchvision import transforms
from PIL import Image
import sys

# Classification ëª¨ë¸ import (ê²½ë¡œ ì¡°ì • í•„ìš”)
sys.path.append(str(Path(__file__).parent / 'ote_velum_classification_final'))
from model import get_model


class IntegratedDISEAnalyzer:
    """í†µí•© DISE ë¹„ë””ì˜¤ ë¶„ì„ê¸°"""
    
    def __init__(self, 
                 model_path,
                 fps_extract=5,
                 threshold_percent=30,
                 exclude_first_seconds=2,
                 exclude_last_seconds=3,
                 min_event_duration=1.0):
        """
        Args:
            model_path: Classification ëª¨ë¸ ê²½ë¡œ
            fps_extract: ì´ˆë‹¹ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜
            threshold_percent: íìƒ‰ ê¸°ì¤€ (%)
            exclude_first_seconds: ì•ë¶€ë¶„ ì œì™¸ (ì´ˆ)
            exclude_last_seconds: ë’·ë¶€ë¶„ ì œì™¸ (ì´ˆ)
            min_event_duration: ìµœì†Œ ì´ë²¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)
        """
        self.fps_extract = fps_extract
        self.threshold_percent = threshold_percent
        self.exclude_first_seconds = exclude_first_seconds
        self.exclude_last_seconds = exclude_last_seconds
        self.min_event_duration = min_event_duration
        
        # Classification ëª¨ë¸ ë¡œë“œ
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self._load_classification_model(model_path)
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        self.class_names = ['None', 'OTE', 'Velum']
        
        self.results = {
            'video_info': {},
            'frame_classifications': [],
            'segments': [],  # OTE/Velum ì—°ì† êµ¬ê°„
            'occlusion_events': [],  # íìƒ‰ ì´ë²¤íŠ¸ (ì—°ì† êµ¬ê°„)
            'summary': {}
        }
    
    def _load_classification_model(self, model_path):
        """Classification ëª¨ë¸ ë¡œë“œ"""
        model = get_model('resnet50', num_classes=3)
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)
        model.eval()
        return model
    
    def preprocess_frame(self, frame):
        """í”„ë ˆì„ ì „ì²˜ë¦¬ - ê²€ì • ë°°ê²½ë§Œ ì œê±° (í¬ë¡­ ì—†ì´)"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        _, binary = cv2.threshold(gray, 20, 255, cv2.THRESH_BINARY)
        
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=3)
        
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            
            # ì—¬ìœ  ë‘ê¸° (ëª¨ì„œë¦¬ ì œê±°)
            margin_percent = 0.075
            margin_x = int(w * margin_percent)
            margin_y = int(h * margin_percent)
            
            x = max(0, x + margin_x)
            y = max(0, y + margin_y)
            w = min(frame.shape[1] - x, w - 2 * margin_x)
            h = min(frame.shape[0] - y, h - 2 * margin_y)
            
            # ì „ì²´ í”„ë ˆì„ í¬ê¸° ìœ ì§€, ê²€ì • ë°°ê²½ë§Œ ë§ˆìŠ¤í‚¹
            result = np.zeros_like(frame)
            result[y:y+h, x:x+w] = frame[y:y+h, x:x+w]
            
            return result, (x, y, w, h)
        
        return frame, (0, 0, frame.shape[1], frame.shape[0])
    
    def classify_frame(self, frame):
        """í”„ë ˆì„ ë¶„ë¥˜ (OTE/Velum/None)"""
        # PIL Imageë¡œ ë³€í™˜
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)
        
        # Transform & ì¶”ë¡ 
        input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probs = torch.softmax(outputs, dim=1)
            pred_class = torch.argmax(probs, dim=1).item()
            confidence = probs[0][pred_class].item()
        
        return self.class_names[pred_class], confidence
    
    def extract_roi_area(self, frame):
        """ROI ì˜ì—­ ë©´ì  ì¶”ì¶œ (ê¸°ë„ ë‚´ë¶€ ì–´ë‘ìš´ ì˜ì—­)"""
        if frame is None or frame.size == 0:
            return 0, None
        
        # HSV ë³€í™˜
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # ì–´ë‘ìš´ ì˜ì—­ íƒ€ê²ŸíŒ…
        lower_dark = np.array([0, 0, 0])
        upper_dark = np.array([180, 255, 80])
        
        mask = cv2.inRange(hsv, lower_dark, upper_dark)
        
        # Morphological operations
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=3)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)
        
        # ê°€ì¥ í° ì—°ê²° ì˜ì—­
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            mask, connectivity=8
        )
        
        if num_labels > 1:
            largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
            area = stats[largest_label, cv2.CC_STAT_AREA]
            mask_final = (labels == largest_label).astype('uint8') * 255
            return float(area), mask_final
        
        return 0, None
    
    def detect_segments(self, frame_classifications):
        """ì—°ì† êµ¬ê°„ ê°ì§€ (OTE/Velum)"""
        segments = []
        current_segment = None
        
        for i, frame_data in enumerate(frame_classifications):
            label = frame_data['label']
            
            if label in ['OTE', 'Velum']:
                if current_segment is None:
                    # ìƒˆ êµ¬ê°„ ì‹œì‘
                    current_segment = {
                        'label': label,
                        'start_frame': frame_data['frame_number'],
                        'start_time': frame_data['timestamp'],
                        'frames': [frame_data]
                    }
                elif current_segment['label'] == label:
                    # ê°™ì€ ë¼ë²¨ ê³„ì†
                    current_segment['frames'].append(frame_data)
                else:
                    # ë¼ë²¨ ë³€ê²½ -> ì´ì „ êµ¬ê°„ ì¢…ë£Œ
                    current_segment['end_frame'] = current_segment['frames'][-1]['frame_number']
                    current_segment['end_time'] = current_segment['frames'][-1]['timestamp']
                    current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
                    segments.append(current_segment)
                    
                    # ìƒˆ êµ¬ê°„ ì‹œì‘
                    current_segment = {
                        'label': label,
                        'start_frame': frame_data['frame_number'],
                        'start_time': frame_data['timestamp'],
                        'frames': [frame_data]
                    }
            else:
                # None -> êµ¬ê°„ ì¢…ë£Œ
                if current_segment is not None:
                    current_segment['end_frame'] = current_segment['frames'][-1]['frame_number']
                    current_segment['end_time'] = current_segment['frames'][-1]['timestamp']
                    current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
                    segments.append(current_segment)
                    current_segment = None
        
        # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
        if current_segment is not None:
            current_segment['end_frame'] = current_segment['frames'][-1]['frame_number']
            current_segment['end_time'] = current_segment['frames'][-1]['timestamp']
            current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
            segments.append(current_segment)
        
        return segments
    
    def detect_occlusion_events(self, segments):
        """íìƒ‰ ì´ë²¤íŠ¸ ê°ì§€ (ì—°ì† êµ¬ê°„)"""
        events = []
        
        for segment in segments:
            # í•´ë‹¹ êµ¬ê°„ì˜ ìµœëŒ€ ROI ë©´ì 
            max_area = max([f.get('roi_area', 0) for f in segment['frames']])
            
            if max_area == 0:
                continue
            
            threshold_area = max_area * (1 - self.threshold_percent / 100)
            
            # íìƒ‰ êµ¬ê°„ ê°ì§€
            current_event = None
            
            for frame_data in segment['frames']:
                roi_area = frame_data.get('roi_area', 0)
                
                if roi_area > 0:
                    area_reduction = (1 - roi_area / max_area) * 100
                    
                    if roi_area < threshold_area:
                        # íìƒ‰ ìƒíƒœ
                        severity = self._classify_severity(area_reduction)
                        
                        if current_event is None:
                            # ìƒˆ ì´ë²¤íŠ¸ ì‹œì‘
                            current_event = {
                                'segment_label': segment['label'],
                                'start_frame': frame_data['frame_number'],
                                'start_time': frame_data['timestamp'],
                                'severity': severity,
                                'max_reduction': area_reduction,
                                'frames': [frame_data]
                            }
                        else:
                            # ì´ë²¤íŠ¸ ê³„ì†
                            current_event['frames'].append(frame_data)
                            current_event['max_reduction'] = max(
                                current_event['max_reduction'], 
                                area_reduction
                            )
                            # ì‹¬ê°ë„ ì—…ë°ì´íŠ¸ (ë” ì‹¬ê°í•œ ê²ƒìœ¼ë¡œ)
                            if self._severity_level(severity) > self._severity_level(current_event['severity']):
                                current_event['severity'] = severity
                    else:
                        # íìƒ‰ í•´ì œ
                        if current_event is not None:
                            current_event['end_frame'] = current_event['frames'][-1]['frame_number']
                            current_event['end_time'] = current_event['frames'][-1]['timestamp']
                            current_event['duration'] = current_event['end_time'] - current_event['start_time']
                            
                            # ìµœì†Œ ì§€ì† ì‹œê°„ ì²´í¬
                            if current_event['duration'] >= self.min_event_duration:
                                events.append(current_event)
                            
                            current_event = None
            
            # ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì²˜ë¦¬
            if current_event is not None:
                current_event['end_frame'] = current_event['frames'][-1]['frame_number']
                current_event['end_time'] = current_event['frames'][-1]['timestamp']
                current_event['duration'] = current_event['end_time'] - current_event['start_time']
                
                if current_event['duration'] >= self.min_event_duration:
                    events.append(current_event)
        
        return events
    
    def _classify_severity(self, reduction_percent):
        """íìƒ‰ ì‹¬ê°ë„ ë¶„ë¥˜"""
        if reduction_percent >= 70:
            return 'Critical'
        elif reduction_percent >= 50:
            return 'Severe'
        elif reduction_percent >= 30:
            return 'Moderate'
        else:
            return 'Mild'
    
    def _severity_level(self, severity):
        """ì‹¬ê°ë„ ìˆ«ì ë³€í™˜"""
        levels = {'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Critical': 4}
        return levels.get(severity, 0)
    
    def create_event_clips(self, video_path, events, output_dir):
        """ìœ„í—˜ êµ¬ê°„ ë¹„ë””ì˜¤ í´ë¦½ ìƒì„±"""
        output_dir = Path(output_dir)
        clips_dir = output_dir / 'event_clips'
        clips_dir.mkdir(parents=True, exist_ok=True)
        
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        clip_paths = []
        
        for i, event in enumerate(events):
            # í´ë¦½ ë²”ìœ„ ì„¤ì • (ì „í›„ 0.5ì´ˆ ì—¬ìœ )
            start_frame = max(0, event['start_frame'] - int(fps * 0.5))
            end_frame = event['end_frame'] + int(fps * 0.5)
            
            clip_filename = f"event_{i+1:03d}_{event['segment_label']}_{event['severity']}.mp4"
            clip_path = clips_dir / clip_filename
            
            # VideoWriter ì„¤ì •
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            out = cv2.VideoWriter(str(clip_path), fourcc, fps, (width, height))
            
            # í”„ë ˆì„ ì¶”ì¶œ ë° ì €ì¥
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            for frame_num in range(start_frame, end_frame + 1):
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ìœ„í—˜ êµ¬ê°„ í‘œì‹œ
                if event['start_frame'] <= frame_num <= event['end_frame']:
                    # ë¹¨ê°„ í…Œë‘ë¦¬
                    cv2.rectangle(frame, (10, 10), (width-10, height-10), (0, 0, 255), 5)
                    
                    # í…ìŠ¤íŠ¸ í‘œì‹œ
                    text = f"{event['severity']} - {event['segment_label']}"
                    cv2.putText(frame, text, (20, 50), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)
                
                out.write(frame)
            
            out.release()
            
            event['clip_path'] = str(clip_path)
            clip_paths.append(clip_path)
        
        cap.release()
        
        return clip_paths
    
    def analyze_video(self, video_path, output_dir=None):
        """
        ë¹„ë””ì˜¤ í†µí•© ë¶„ì„
        
        Returns:
            results: {
                'video_info': {...},
                'frame_classifications': [...],
                'segments': [...],
                'occlusion_events': [...],
                'summary': {...}
            }
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        # ë¹„ë””ì˜¤ ì •ë³´
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0.0
        
        # ì²˜ë¦¬ ë²”ìœ„
        exclude_first_frames = int(fps * self.exclude_first_seconds)
        exclude_last_frames = int(fps * self.exclude_last_seconds)
        start_frame = exclude_first_frames
        end_frame = total_frames - exclude_last_frames
        
        self.results['video_info'] = {
            'filename': video_path.name,
            'fps': fps,
            'total_frames': total_frames,
            'duration': duration,
            'analyzed_range': [start_frame, end_frame]
        }
        
        print("\n" + "="*70)
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ë¶„ì„: {video_path.name}")
        print("="*70)
        print(f"  - FPS: {fps:.2f}")
        print(f"  - ì´ í”„ë ˆì„: {total_frames} ({duration:.1f}ì´ˆ)")
        print(f"  - ë¶„ì„ êµ¬ê°„: {start_frame} ~ {end_frame}")
        
        # í”„ë ˆì„ ì¶”ì¶œ ê°„ê²©
        frame_interval = max(1, int(fps / self.fps_extract))
        
        # 1ë‹¨ê³„: í”„ë ˆì„ë³„ ë¶„ë¥˜ + ROI ë¶„ì„
        print("\nğŸ” 1ë‹¨ê³„: í”„ë ˆì„ ë¶„ë¥˜ ë° ROI ë¶„ì„...")
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        frame_count = start_frame
        
        pbar = tqdm(total=(end_frame - start_frame), desc="í”„ë ˆì„ ì²˜ë¦¬")
        
        while frame_count < end_frame:
            ret, frame = cap.read()
            if not ret:
                break
            
            if (frame_count - start_frame) % frame_interval == 0:
                # ì „ì²˜ë¦¬
                preprocessed, bbox = self.preprocess_frame(frame)
                
                # ë¶„ë¥˜
                label, confidence = self.classify_frame(preprocessed)
                
                # ROI ë©´ì  (OTE/Velumì¸ ê²½ìš°ë§Œ)
                roi_area = 0
                if label in ['OTE', 'Velum']:
                    roi_area, _ = self.extract_roi_area(preprocessed)
                
                timestamp = frame_count / fps
                
                frame_data = {
                    'frame_number': frame_count,
                    'timestamp': timestamp,
                    'time_str': str(timedelta(seconds=int(timestamp))),
                    'label': label,
                    'confidence': confidence,
                    'roi_area': roi_area,
                    'bbox': bbox
                }
                
                self.results['frame_classifications'].append(frame_data)
            
            frame_count += 1
            pbar.update(1)
        
        pbar.close()
        cap.release()
        
        # 2ë‹¨ê³„: ì—°ì† êµ¬ê°„ ê°ì§€
        print("\nğŸ” 2ë‹¨ê³„: ì—°ì† êµ¬ê°„ ê°ì§€...")
        self.results['segments'] = self.detect_segments(
            self.results['frame_classifications']
        )
        
        print(f"  âœ“ ì´ {len(self.results['segments'])}ê°œ êµ¬ê°„ ê°ì§€")
        for seg in self.results['segments']:
            print(f"    - {seg['label']}: {seg['start_time']:.1f}s ~ {seg['end_time']:.1f}s "
                  f"({seg['duration']:.1f}s)")
        
        # 3ë‹¨ê³„: íìƒ‰ ì´ë²¤íŠ¸ ê°ì§€
        print("\nğŸ” 3ë‹¨ê³„: íìƒ‰ ì´ë²¤íŠ¸ ê°ì§€...")
        self.results['occlusion_events'] = self.detect_occlusion_events(
            self.results['segments']
        )
        
        print(f"  âœ“ ì´ {len(self.results['occlusion_events'])}ê°œ ì´ë²¤íŠ¸ ê°ì§€")
        for i, event in enumerate(self.results['occlusion_events'], 1):
            print(f"    #{i} {event['segment_label']} - {event['severity']}: "
                  f"{event['start_time']:.1f}s ~ {event['end_time']:.1f}s "
                  f"({event['duration']:.1f}s, {event['max_reduction']:.1f}% ê°ì†Œ)")
        
        # 4ë‹¨ê³„: ë¹„ë””ì˜¤ í´ë¦½ ìƒì„±
        if output_dir and self.results['occlusion_events']:
            print("\nğŸ” 4ë‹¨ê³„: ìœ„í—˜ êµ¬ê°„ ë¹„ë””ì˜¤ í´ë¦½ ìƒì„±...")
            clip_paths = self.create_event_clips(
                video_path, 
                self.results['occlusion_events'], 
                output_dir
            )
            print(f"  âœ“ {len(clip_paths)}ê°œ í´ë¦½ ìƒì„± ì™„ë£Œ")
        
        # ìš”ì•½ ì •ë³´
        self.results['summary'] = {
            'total_segments': len(self.results['segments']),
            'ote_segments': sum(1 for s in self.results['segments'] if s['label'] == 'OTE'),
            'velum_segments': sum(1 for s in self.results['segments'] if s['label'] == 'Velum'),
            'total_events': len(self.results['occlusion_events']),
            'events_by_severity': {
                'Critical': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Critical'),
                'Severe': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Severe'),
                'Moderate': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Moderate'),
                'Mild': sum(1 for e in self.results['occlusion_events'] if e['severity'] == 'Mild'),
            }
        }
        
        # JSON ì €ì¥
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            results_file = output_path / 'analysis_results.json'
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nğŸ“„ ê²°ê³¼ ì €ì¥: {results_file}")
        
        print("\n" + "="*70)
        print("âœ… ë¶„ì„ ì™„ë£Œ!")
        print("="*70)
        
        return self.results


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    analyzer = IntegratedDISEAnalyzer(
        model_path='checkpoints/best_model.pth',
        fps_extract=5,
        threshold_percent=30,
        min_event_duration=1.0
    )
    
    results = analyzer.analyze_video(
        video_path='test_video.mp4',
        output_dir='analysis_output'
    )
    
    print(f"\nğŸ“Š ìš”ì•½:")
    print(f"  - ì´ êµ¬ê°„: {results['summary']['total_segments']}ê°œ")
    print(f"  - íìƒ‰ ì´ë²¤íŠ¸: {results['summary']['total_events']}ê°œ")
