"""
OTE/Velum/None ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- video_analyzer.pyì˜ AirwayOcclusionAnalyzer í™œìš©
- í’ˆì§ˆ ê¸°ë°˜ ìë™ ë¶„ë¥˜
- í”„ë ˆì„ ì¶”ì¶œ ë° ë ˆì´ë¸”ë§
"""

import cv2
import numpy as np
from pathlib import Path
import json
from tqdm import tqdm
import sys

# ìƒìœ„ í´ë”ë¥¼ Python pathì— ì¶”ê°€
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

# video_analyzerì˜ AirwayOcclusionAnalyzer import
from video_analyzer import AirwayOcclusionAnalyzer


class DISEDatasetPreprocessor:
    """
    DISE ë¹„ë””ì˜¤ë¥¼ OTE/Velum/Noneìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ ë°ì´í„°ì…‹ ìƒì„±
    """
    
    def __init__(self, dataset_path='dataset', output_path='processed_dataset'):
        self.dataset_path = Path(dataset_path)
        self.output_path = Path(output_path)
        self.output_path.mkdir(exist_ok=True)
        
        # í´ë˜ìŠ¤ë³„ ì¶œë ¥ í´ë”
        for class_name in ['OTE', 'Velum', 'None']:
            (self.output_path / class_name).mkdir(exist_ok=True)
        
        self.all_annotations = []
    
    def analyze_frame_quality(self, frame):
        """
        í”„ë ˆì„ í’ˆì§ˆ ë¶„ì„
        
        Returns:
            quality_score: 0~1 ì‚¬ì´ì˜ í’ˆì§ˆ ì ìˆ˜
            metrics: ê° ì§€í‘œë³„ ê°’
        """
        if frame is None or frame.size == 0:
            return 0.0, {}
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # 1. ë°ê¸°
        brightness = np.mean(gray) / 255.0
        
        # 2. ì„ ëª…ë„ (Laplacian variance)
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        sharpness = min(laplacian.var() / 1000.0, 1.0)
        
        # 3. ëŒ€ë¹„
        contrast = np.std(gray) / 128.0
        
        # 4. ì—£ì§€ ë°€ë„
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # 5. ìƒ‰ìƒ ë¶„ì‚°
        color_std = np.mean([np.std(frame[:,:,i]) for i in range(3)]) / 128.0
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
        quality_score = (
            0.25 * brightness +
            0.25 * sharpness +
            0.20 * contrast +
            0.15 * edge_density +
            0.15 * color_std
        )
        
        metrics = {
            'brightness': brightness,
            'sharpness': sharpness,
            'contrast': contrast,
            'edge_density': edge_density,
            'color_std': color_std,
            'quality_score': quality_score
        }
        
        return quality_score, metrics
    

# preprocess_with_analyzer.py ë‚´ë¶€ ë©”ì„œë“œ ìˆ˜ì •

    def is_tissue_color(self, frame):
        """
        í”„ë ˆì„ì´ ì¸ì²´ ì¡°ì§(ë¶‰ì€ìƒ‰/ë¶„í™ìƒ‰ ê³„ì—´)ì¸ì§€ í™•ì¸
        """
        # BGR í‰ê·  ê³„ì‚°
        b_mean = np.mean(frame[:, :, 0])
        g_mean = np.mean(frame[:, :, 1])
        r_mean = np.mean(frame[:, :, 2])
        
        # 1. ë¶‰ì€ìƒ‰ì´ íŒŒë€ìƒ‰/ì´ˆë¡ìƒ‰ë³´ë‹¤ ìš°ì„¸í•´ì•¼ í•¨ (ì¡°ì§ íŠ¹ì„±)
        is_red_dominant = (r_mean > g_mean) and (r_mean > b_mean)
        
        # 2. ì ì ˆí•œ ì±„ë„ê°€ ìˆì–´ì•¼ í•¨ (íšŒìƒ‰ì¡° ë…¸ì´ì¦ˆ ì œì™¸)
        # Rê³¼ G/Bì˜ ì°¨ì´ê°€ ì¼ì • ìˆ˜ì¤€ ì´ìƒ
        color_diff = r_mean - ((g_mean + b_mean) / 2)
        
        return is_red_dominant and (color_diff > 5)

    def classify_frame(self, frame, roi_area, quality_score, video_type,
                    max_roi_area=None, metrics=None):
        """
        [ìˆ˜ì • ë²„ì „ 7: ì¹¨/ê±°í’ˆ(Saliva) ì €ê²© íŒ¨ì¹˜]
        - ì •ìƒ OTEëŠ” ë§¤ë„ëŸ½ì§€ë§Œ, ì¹¨/ê±°í’ˆì€ 'ìê¸€ìê¸€'í•˜ë‹¤ëŠ” ì ì„ ì´ìš©
        - Edge Scoreê°€ 'ë„ˆë¬´ ë†’ìœ¼ë©´' ë…¸ì´ì¦ˆ(ê±°í’ˆ)ë¡œ ê°„ì£¼í•˜ì—¬ ì œê±°
        """
        
        # ê³µí†µ ê¸°ì´ˆ í†µê³„ëŸ‰
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        mean_brightness = np.mean(gray)
        
        # 1. ì ˆëŒ€ ë°©ì¶œ ê¸°ì¤€
        if mean_brightness < 5: return 'None', 0.99
        if mean_brightness > 250: return 'None', 0.99

        # ---------------------------------------------------------
        # 2. [OTE ì „ìš©] ì¹¨/ê±°í’ˆ ì €ê²© & êµ¬ì¡° ê²€ì‚¬
        # ---------------------------------------------------------
        if video_type == 'OTE':
            # (1) Canny Edge ê³„ì‚°
            # ë¯¼ê°ë„(20, 80) ìœ ì§€í•˜ë˜, ê²°ê³¼ë¥¼ í•´ì„í•˜ëŠ” ë°©ë²•ì„ ë°”ê¿ˆ
            edges = cv2.Canny(gray, 20, 80)
            edge_score = np.mean(edges)
            
            # (2) [ì‹ ê·œ] ì¹¨/ê±°í’ˆ í•¨ì • (Saliva Trap) ğŸ•¸ï¸
            # ê±°í’ˆì€ í…Œë‘ë¦¬ê°€ ë§ì•„ì„œ Edge Scoreê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ê²Œ ë‚˜ì˜´ (ë³´í†µ 10~15 ì´ìƒ)
            # ë°˜ë©´ ì •ìƒ ê¸°ë„ëŠ” ë§¤ë„ëŸ¬ì›Œì„œ ë³´í†µ 3~8 ì‚¬ì´ê°€ ë‚˜ì˜´.
            # ROIê°€ ì´ˆëŒ€í˜•(>3000)ì´ ì•„ë‹Œë° Edgeê°€ ë„ˆë¬´ ë§ìœ¼ë©´ -> 100% ê±°í’ˆì„
            if edge_score > 12.0 and roi_area < 3000:
                return 'None', 0.95

            # (3) [ì‹ ê·œ] ë°˜ì‚¬ê´‘(Specular Highlight) ì €ê²©
            # ì¹¨ë°©ìš¸ì€ ë¹›ì„ ë°˜ì‚¬í•´ êµ­ì†Œì ìœ¼ë¡œ ì—„ì²­ ë°ê³  ì„ ëª…í•¨
            # ì„ ëª…ë„(Sharpness)ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ë° ROIëŠ” ì‘ë‹¤? -> ì¹¨ë°©ìš¸
            sharpness = metrics.get('sharpness', 0) if metrics else 0
            if sharpness > 0.05 and roi_area < 1000:  # 0.05ëŠ” ê½¤ ë†’ì€ ìˆ˜ì¹˜
                return 'None', 0.90

            # (4) êµ¬ë©(ROI) ìš°ì„  êµ¬ì œ (V6 ë¡œì§ ìœ ì§€)
            # ê±°í’ˆì´ ìˆì–´ë„ ì§„ì§œ ê¸°ë„ê°€ ë»¥ ëš«ë ¤ìˆìœ¼ë©´(>1000) OTEë¡œ ì¸ì •
            if roi_area > 1000:
                return 'OTE', 0.95

            # (5) ìµœì†Œ ì—£ì§€ ì ìˆ˜ ì‹¬ì‚¬ (V6 ìœ ì§€)
            # ë„ˆë¬´ ë§¹íƒ•(ì•ˆê°œ)ì¸ ê²ƒë§Œ ì œê±°
            if edge_score < 0.5:
                if roi_area < 200: return 'None', 0.95
                
            # (6) ì¡°ì§ ìƒ‰ìƒ ê²€ì‚¬
            is_tissue = self.is_tissue_color(frame)
            if not is_tissue and roi_area < 500:
                return 'None', 0.85

            return 'OTE', 0.85

        # ---------------------------------------------------------
        # 3. [Velum ì „ìš©] ê¸°ì¡´ ìœ ì§€
        # ---------------------------------------------------------
        else:
            contrast = metrics.get('contrast', 0) if metrics else 0
            if contrast < 0.02: return 'None', 0.95
            
            is_tissue = self.is_tissue_color(frame)
            sharpness = metrics.get('sharpness', 0) if metrics else 0
            
            if is_tissue and sharpness < 0.001:
                return 'None', 0.90

            if roi_area < 50 and quality_score < 0.10:
                return 'None', 0.85

            confidence = 0.8
            if roi_area > 500: confidence += 0.15
            if is_tissue: confidence += 0.15
            
            if roi_area < 50 and not is_tissue and mean_brightness < 40:
                return 'None', 0.85
                
            return 'Velum', min(confidence, 1.0)

    def process_video_with_analyzer(self, video_path, video_type):
        """
        video_analyzerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ì²˜ë¦¬
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            video_type: 'OTE' or 'Velum'
        """
        print(f"\n{'='*70}")
        print(f"Processing: {video_path.name} (Type: {video_type})")
        print(f"{'='*70}")
        
        # AirwayOcclusionAnalyzer ìƒì„±
        analyzer = AirwayOcclusionAnalyzer(
            fps_extract=5,  # ì´ˆë‹¹ 5í”„ë ˆì„ ì¶”ì¶œ
            threshold_percent=30,
            exclude_first_seconds=2,
            exclude_last_seconds=3
        )
        
        # ë¶„ì„ ì‹¤í–‰
        results = analyzer.analyze_video(str(video_path), output_dir=None)
        
        # â­ ìµœëŒ€ ROI ë©´ì  (ì „ì²´ í”„ë ˆì„ ì¤‘)
        max_roi_area = results['max_area']
        
        print(f"\nğŸ“Š ë¹„ë””ì˜¤ ROI í†µê³„:")
        print(f"  - ìµœëŒ€ ROI ë©´ì : {max_roi_area:.0f} pxÂ²")
        
        # ì´ì œ ë‹¤ì‹œ í”„ë ˆì„ë³„ë¡œ ë¶„ë¥˜
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        video_name = video_path.stem
        frame_count = 0
        saved_count = {'OTE': 0, 'Velum': 0, 'None': 0}
        
        print(f"\nClassifying and saving frames...")
        
        for frame_data in tqdm(results['frames'], desc="Frames"):
            frame_number = frame_data['frame_number']
            roi_area = frame_data.get('roi_area', 0)
            
            # í”„ë ˆì„ ì½ê¸°
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = cap.read()
            
            if not ret:
                continue
            
            # í”„ë ˆì„ ì „ì²˜ë¦¬ (analyzerì™€ ë™ì¼)
            preprocessed, bbox = analyzer.preprocess_frame(frame)
            
            # í’ˆì§ˆ ë¶„ì„
            quality_score, metrics = self.analyze_frame_quality(preprocessed)
            
            # ë¶„ë¥˜
            label, confidence = self.classify_frame(
                preprocessed, roi_area, quality_score,
                video_type, max_roi_area, metrics
            )
            
            # í”„ë ˆì„ ì €ì¥
            output_folder = self.output_path / label
            frame_filename = f"{video_name}_{label}_frame_{saved_count[label]:06d}.jpg"
            frame_path = output_folder / frame_filename
            
            cv2.imwrite(str(frame_path), preprocessed)
            
            # ì–´ë…¸í…Œì´ì…˜ ì €ì¥
            annotation = {
                'filename': frame_filename,
                'label': label,
                'video_name': video_name,
                'video_type': video_type,
                'frame_number': frame_number,
                'timestamp': frame_data['timestamp'],
                'roi_area': roi_area,
                'quality_score': quality_score,
                'confidence': confidence,
                'metrics': metrics,
                'path': str(frame_path)
            }
            
            self.all_annotations.append(annotation)
            saved_count[label] += 1
        
        cap.release()
        
        # í†µê³„ ì¶œë ¥
        print(f"\nâœ“ {video_path.name} ì²˜ë¦¬ ì™„ë£Œ")
        print(f"  - OTE: {saved_count['OTE']} frames")
        print(f"  - Velum: {saved_count['Velum']} frames")
        print(f"  - None: {saved_count['None']} frames")
        
        return saved_count
    
    def process_all_videos(self):
        """ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        total_stats = {'OTE': 0, 'Velum': 0, 'None': 0}
        
        # Velum ë¹„ë””ì˜¤ ì²˜ë¦¬
        velum_path = self.dataset_path / 'Velum'
        if velum_path.exists():
            velum_videos = list(velum_path.glob('*.mp4'))
            print(f"\nğŸ“¹ Found {len(velum_videos)} Velum videos")
            
            for video_file in velum_videos:
                stats = self.process_video_with_analyzer(video_file, 'Velum')
                for key in total_stats:
                    total_stats[key] += stats[key]
        
        # OTE ë¹„ë””ì˜¤ ì²˜ë¦¬
        ote_path = self.dataset_path / 'OTE'
        if ote_path.exists():
            ote_videos = list(ote_path.glob('*.mp4'))
            print(f"\nğŸ“¹ Found {len(ote_videos)} OTE videos")
            
            for video_file in ote_videos:
                stats = self.process_video_with_analyzer(video_file, 'OTE')
                for key in total_stats:
                    total_stats[key] += stats[key]
        
        return total_stats
    
    def save_annotations(self):
        """ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì €ì¥"""
        annotation_file = self.output_path / 'annotations.json'
        
        with open(annotation_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_annotations, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ Annotations saved: {annotation_file}")
        
        # í†µê³„ íŒŒì¼ ìƒì„±
        stats = {
            'total_frames': len(self.all_annotations),
            'class_distribution': {},
            'video_types': {}
        }
        
        for ann in self.all_annotations:
            label = ann['label']
            video_type = ann['video_type']
            
            stats['class_distribution'][label] = stats['class_distribution'].get(label, 0) + 1
            stats['video_types'][video_type] = stats['video_types'].get(video_type, 0) + 1
        
        stats_file = self.output_path / 'dataset_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š Statistics saved: {stats_file}")
        
        return stats
    
    def run(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("\n" + "="*70)
        print("DISE Dataset Preprocessing Pipeline")
        print("="*70)
        
        # ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬
        total_stats = self.process_all_videos()
        
        # ì–´ë…¸í…Œì´ì…˜ ì €ì¥
        stats = self.save_annotations()
        
        # ìµœì¢… í†µê³„
        print("\n" + "="*70)
        print("=== Final Statistics ===")
        print("="*70)
        print(f"Total frames: {stats['total_frames']}")
        print(f"\nClass distribution:")
        for label, count in stats['class_distribution'].items():
            percentage = count / stats['total_frames'] * 100
            print(f"  {label}: {count} frames ({percentage:.1f}%)")
        
        print(f"\nVideo types:")
        for vtype, count in stats['video_types'].items():
            print(f"  {vtype}: {count} frames")
        
        print("\n" + "="*70)
        print("âœ… Preprocessing completed!")
        print("="*70)
        
        return stats


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='DISE Video Dataset Preprocessing'
    )
    parser.add_argument('--dataset', type=str, default='D:/chaeyeon/ëŒ€í•™ì›/3í•™ê¸°/ìœµí”„/data/DISE_DATA(AIHub)/little',
                       help='Dataset directory containing OTE/ and Velum/ folders')
    parser.add_argument('--output', type=str, default='processed_dataset',
                       help='Output directory for processed frames')
    
    args = parser.parse_args()
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    preprocessor = DISEDatasetPreprocessor(
        dataset_path=args.dataset,
        output_path=args.output
    )
    
    stats = preprocessor.run()
    
    print(f"\nğŸ’¡ Next steps:")
    print(f"1. Check the frames in {args.output}/[OTE|Velum|None]/")
    print(f"2. Review dataset_stats.json for class distribution")
    print(f"3. Run: python train.py")


if __name__ == '__main__':
    main()
